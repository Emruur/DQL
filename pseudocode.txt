initialize Q network
initialize Q_target as a copy of Q_sa
initialize 

sample initial state

while budget do:
    if at mth iteration
        Q_target <- Q
    sample action
    r,s_next <- simulate env

    store the experience (s,a,s_next,r)
    sample n samples from replay buffer
    $$
        Q^{\theta}(s, a) \leftarrow Q^{\theta}(s, a) + \alpha \left[ r + \gamma \max_{a'} Q^{\theta^-}(s', a') - Q^{\theta}(s, a) \right]
    $$



    \documentclass{article}
\usepackage{amsmath}

\begin{document}

\noindent
\textbf{Algorithm: Deep Q-Learning with Target Network and Replay Buffer}

\textbf{Input:} Learning rate $\alpha$, discount factor $\gamma$, update interval $m$, replay buffer size $N$, total budget.

\begin{enumerate}
    \item Initialize Q-network $Q^{\theta}(s, a)$
    \item Initialize target network $Q^{\theta^-}(s, a) \gets Q^{\theta}(s, a)$
    \item Initialize replay buffer $\mathcal{D}$
    \item Sample initial state $s$
    \item \textbf{while} budget do:
    \begin{enumerate}
        \item \textbf{if} at $m^{th}$ iteration:
        \begin{enumerate}
            \item $Q^{\theta^-} \gets Q^{\theta}$ \hfill /* Update target network */
        \end{enumerate}
        \item Sample action $a$ using policy (e.g., $\epsilon$-greedy)
        \item Simulate environment to obtain reward and next state: $r, s' \sim p(r, s' | s, a)$
        \item Store experience $(s, a, s', r)$ in replay buffer $\mathcal{D}$
        \item Sample $n$ experiences from replay buffer
        \item Perform Q-update using sampled batch:
        \begin{equation}
        Q^{\theta}(s, a) \leftarrow Q^{\theta}(s, a) + \alpha \left[ r + \gamma \max_{a'} Q^{\theta^-}(s', a') - Q^{\theta}(s, a) \right]
        \end{equation}
        \item Set $s \gets s'$
    \end{enumerate}
    \item \textbf{end}
\end{enumerate}

\noindent
\textbf{Return:} $Q^{\theta}(s, a)$

\end{document}
